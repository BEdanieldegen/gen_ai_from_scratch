{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "#### https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n",
    "#### https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  torch.set_printoptions(precision=8)\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.32059336, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1\n",
    "### Find Derivatives through the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: False | approximate: False | maxdiff: 0.005610490683466196\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bndiff          | exact: False | approximate: False | maxdiff: 0.0011830269359052181\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "hprebn          | exact: False | approximate: False | maxdiff: 0.001010074745863676\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# Manually (own solution)\n",
    "\n",
    "# dy/dx \n",
    "# dloss/dlogprobs\n",
    "# loss = -1 * (a + b + c + ... + k) / n\n",
    "# loss = -1/n*a + -1/n*b + -1/n*c ... + -1/n*k\n",
    "# dloss/da = -1/n\n",
    "# dloss/db = -1/n\n",
    "# ...\n",
    "# \n",
    "dlogprobs =  torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1/n\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "# dloss/dprobs\n",
    "# Local derivative:\n",
    "# dL/dprobs: log(probs)  |  log(x): 1/x\n",
    "# 1/probs\n",
    "# Chainrule:\n",
    "# 1/probs*dlogprobs\n",
    "\n",
    "dprobs = 1/probs * dlogprobs\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "\n",
    "# dloss/dcounts_sum_inv\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# -1 * counts_sum**-2 * dprobs\n",
    "\n",
    "dcounts_sum_inv = torch.zeros_like(counts_sum_inv)\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "#Can't check here since counce sum inv depends on counts which I'll do later\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "dcounts_sum = -1 *counts_sum**-2 * dcounts_sum_inv\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "dnorm_logits= norm_logits.exp() * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "\n",
    "dlogits = dnorm_logits.clone()\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes= logits.shape[1]) * dlogit_maxes\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "#logits = h @ W2 + b2 # output layer\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "dhpreact = (1-torch.tanh(hpreact)**2) * dh\n",
    "#dhpreact = (1.0-h**2) * dh   # equivalent\n",
    "\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "\n",
    "\n",
    "#hpreact = bngain * bnraw + bnbias\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "#hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "\n",
    "#bndiff2 = bndiff**2\n",
    "#bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw # noch nicht korrekt: bndiff geht noch in bndiff2 rein\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "#dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff2 = 1/(n-1)  * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "\n",
    "#emb = C[Xb] # embed the characters into vectors\n",
    "#embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "#hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "dhprebn = dbndiff.clone() # something is still missing\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "dhprebn += torch.ones_like(hprebn) * 1/n * dbnmeani\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "\n",
    "#emb = C[Xb] # embed the characters into vectors\n",
    "#embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "demb = dembcat.view(emb.shape) # put back in old shape\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]): # iterate over rows: 32\n",
    "    for j in range(Xb.shape[1]): # iterate over columns: 3\n",
    "        ix = Xb[i,j] \n",
    "        dC[ix] += demb[i, j]\n",
    "        #print(i, j)\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3205933570861816 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "dlogits = F.softmax(logits, 1)\n",
    "\n",
    "# equivalent\n",
    "dlogits_ = torch.zeros(32,27)\n",
    "for i in range(len(dlogits_)):\n",
    "    for j in range(len(dlogits_[0])):\n",
    "        dlogits_ = logits.exp() / logits.exp().sum(1, keepdims=True)\n",
    "\n",
    "dlogits_[range(n), Yb] -= 1\n",
    "dlogits_ /= n\n",
    "len(dlogits_)\n",
    "\n",
    "#dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n"
     ]
    }
   ],
   "source": [
    "cmp('logits', dlogits_, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison: We push the probility of the correct value to one and pull down the probablities of the incorrect values to be zero\n",
    "#### The gradient is the effect on the loss. Intuitively, the correct value should have a negative effect on the loss (loss is minimized), while the other character should not have any effect at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.32830644e-10, -3.25962901e-09, -3.72529030e-09, -2.79396772e-09,\n",
       "         1.39698386e-09, -1.86264515e-09, -1.39698386e-09,  3.25962901e-09,\n",
       "        -2.79396772e-09, -1.16415322e-09, -9.31322575e-10,  3.72529030e-09,\n",
       "         9.31322575e-10, -3.25962901e-09, -1.16415322e-09, -3.49245965e-09,\n",
       "         9.31322575e-10,  2.79396772e-09, -9.31322575e-10, -2.32830644e-09,\n",
       "        -4.65661287e-10, -1.62981451e-09, -9.31322575e-10, -1.62981451e-09,\n",
       "        -1.86264515e-09,  0.00000000e+00,  2.32830644e-10,  1.16415322e-09,\n",
       "        -2.79396772e-09,  1.39698386e-09, -1.86264515e-09,  1.62981451e-09],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_.sum(1) # row sums are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.07736547,  0.08558806,  0.01605983,  0.04424230,  0.01830548,\n",
       "         0.08165498,  0.02398051,  0.03947157, -0.98193550,  0.03381820,\n",
       "         0.03634686,  0.04121972,  0.03327779,  0.02725406,  0.03191340,\n",
       "         0.01387656,  0.00887026,  0.02066619,  0.01623388,  0.05009785,\n",
       "         0.05933364,  0.02194253,  0.02402211,  0.06971362,  0.05699923,\n",
       "         0.02569012,  0.02399129], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits_[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6e1015a2f0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits_.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
