{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations & Gradients, BatchNorm\n",
    "\n",
    "# https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb\n",
    "# https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up\n",
    "##### as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) # * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) # * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) #* 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) #* 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "  10000/ 200000: 2.9184\n",
      "  20000/ 200000: 2.5847\n",
      "  30000/ 200000: 2.7859\n",
      "  40000/ 200000: 2.0222\n",
      "  50000/ 200000: 2.6171\n",
      "  60000/ 200000: 2.3168\n",
      "  70000/ 200000: 2.0853\n",
      "  80000/ 200000: 2.2519\n",
      "  90000/ 200000: 2.2718\n",
      " 100000/ 200000: 2.0234\n",
      " 110000/ 200000: 2.4689\n",
      " 120000/ 200000: 1.9426\n",
      " 130000/ 200000: 2.3817\n",
      " 140000/ 200000: 2.3213\n",
      " 150000/ 200000: 2.1416\n",
      " 160000/ 200000: 2.2320\n",
      " 170000/ 200000: 1.7736\n",
      " 180000/ 200000: 2.0826\n",
      " 190000/ 200000: 1.8465\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1276261806488037\n",
      "val 2.1701834201812744\n",
      "train 2.1276261806488037\n",
      "val 2.1701834201812744\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h =torch.tanh(embcat @ W1 + b1)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "  return loss.item()\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "tracking_stats = ['Model 1',split_loss('train'),split_loss('val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mona.\n",
      "mayah.\n",
      "seel.\n",
      "nah.\n",
      "yam.\n",
      "rethrsonngramira.\n",
      "eredielin.\n",
      "shy.\n",
      "jen.\n",
      "eden.\n",
      "estanaraelyn.\n",
      "malka.\n",
      "cayshubergihamier.\n",
      "kendreelynn.\n",
      "nochorius.\n",
      "mace.\n",
      "ryyah.\n",
      "fael.\n",
      "yumaje.\n",
      "sanyah.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 3\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple things are wrong with the modeL:\n",
    "##### a. First loglikelihood is very high: what initial loss would you expect? given a uniform distribution\n",
    "##### So at initialization we should plug in a uniform distribution, so every character is equally likely\n",
    "##### -> this means setting b2 to zero\n",
    "##### -> setting W2 close to zero: multiply by 0.01 or so: why not set it to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected probability is 1/27\n",
    "-torch.tensor(1.0/27.0).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very high loss: 6.003401279449463  given the probabilities: tensor([9.9660e-01, 1.6645e-05, 2.4703e-03, 9.0879e-04])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([7.0,-4.0,1.0,0.0])\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[2].log()\n",
    "print(\"Very high loss:\", loss.item(), \" given the probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First guess of the loss would be: 1.3862943649291992 given probs: tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([0.0,0.0,0.0,0.0]) # should be set to equal in the initialization\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[2].log()\n",
    "print(\"First guess of the loss would be:\", loss.item(), \"given probs:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so here is the reviseted code: b2 = b2 * 0; W2 = W2 * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n",
      "11897\n",
      "      0/ 200000: 3.3148\n",
      "  10000/ 200000: 2.6505\n",
      "  20000/ 200000: 2.4705\n",
      "  30000/ 200000: 2.0266\n",
      "  40000/ 200000: 2.1797\n",
      "  50000/ 200000: 2.4072\n",
      "  60000/ 200000: 2.2316\n",
      "  70000/ 200000: 2.0701\n",
      "  80000/ 200000: 2.0010\n",
      "  90000/ 200000: 2.0439\n",
      " 100000/ 200000: 2.4564\n",
      " 110000/ 200000: 2.2124\n",
      " 120000/ 200000: 2.1843\n",
      " 130000/ 200000: 2.2414\n",
      " 140000/ 200000: 2.1447\n",
      " 150000/ 200000: 2.2837\n",
      " 160000/ 200000: 2.0286\n",
      " 170000/ 200000: 2.0146\n",
      " 180000/ 200000: 2.2178\n",
      " 190000/ 200000: 2.0372\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) # * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) # * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.069324493408203\n",
      "val 2.133209228515625\n",
      "train 2.069324493408203\n",
      "val 2.133209228515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Model 1', 2.1276261806488037, 2.1701834201812744],\n",
       " ['Model 2:  b2*0; W2=W2*0.01', 2.069324493408203, 2.133209228515625])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h =torch.tanh(embcat @ W1 + b1)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "  return loss.item()\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "tracking_stats = tracking_stats , ['Model 2:  b2*0; W2=W2*0.01',split_loss('train'),split_loss('val')]\n",
    "tracking_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Solved loss with respect to logits, BUT Still a problem with initialization \n",
    "#### look at values of h (activations of hidden states) -> many values are 1 or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preactivation\n",
    "#plt.hist(hpreact.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation\n",
    "#plt.hist(h.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is problematic!\n",
    "\n",
    "#### depending on the function where you have flat areas (as in tanh), the neurons could by chance end up to be 1 and -1 (or 1 and 0). So these neurons never learn: permanent brain damage in the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distirbution with a larger variance (mean = zero)\n",
    "a = torch.empty(10000).normal_(mean=0,std=1.5)\n",
    "#plt.hist(a.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = torch.tanh(a)\n",
    "#plt.hist(a2.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182437, 3]) torch.Size([182437])\n",
      "torch.Size([22781, 3]) torch.Size([22781])\n",
      "torch.Size([22928, 3]) torch.Size([22928])\n",
      "11897\n",
      "      0/ 200000: 3.2959\n",
      "  10000/ 200000: 1.8335\n",
      "  20000/ 200000: 1.9758\n",
      "  30000/ 200000: 2.4127\n",
      "  40000/ 200000: 2.5336\n",
      "  50000/ 200000: 1.8456\n",
      "  60000/ 200000: 2.4327\n",
      "  70000/ 200000: 2.0982\n",
      "  80000/ 200000: 1.8181\n",
      "  90000/ 200000: 2.2853\n",
      " 100000/ 200000: 2.1659\n",
      " 110000/ 200000: 2.2114\n",
      " 120000/ 200000: 2.0398\n",
      " 130000/ 200000: 1.8429\n",
      " 140000/ 200000: 2.2416\n",
      " 150000/ 200000: 2.1392\n",
      " 160000/ 200000: 1.8348\n",
      " 170000/ 200000: 2.1022\n",
      " 180000/ 200000: 2.5763\n",
      " 190000/ 200000: 1.8410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([587., 110.,  46.,  55.,  37.,  31.,  38.,  33.,  26.,  36.,  26.,\n",
       "         32.,  24.,  29.,  41.,  45.,  37.,  55.,  73.,  83., 138., 228.,\n",
       "        296., 455., 615., 627., 423., 284., 207., 141., 139.,  64.,  73.,\n",
       "         31.,  26.,  36.,  34.,  22.,  28.,  38.,  39.,  19.,  35.,  34.,\n",
       "         37.,  51.,  54.,  59., 102., 621.]),\n",
       " array([-1.  , -0.96, -0.92, -0.88, -0.84, -0.8 , -0.76, -0.72, -0.68,\n",
       "        -0.64, -0.6 , -0.56, -0.52, -0.48, -0.44, -0.4 , -0.36, -0.32,\n",
       "        -0.28, -0.24, -0.2 , -0.16, -0.12, -0.08, -0.04,  0.  ,  0.04,\n",
       "         0.08,  0.12,  0.16,  0.2 ,  0.24,  0.28,  0.32,  0.36,  0.4 ,\n",
       "         0.44,  0.48,  0.52,  0.56,  0.6 ,  0.64,  0.68,  0.72,  0.76,\n",
       "         0.8 ,  0.84,  0.88,  0.92,  0.96,  1.  ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.01\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "# Activation\n",
    "plt.hist(h.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1016416549682617\n",
      "val 2.151332139968872\n",
      "train 2.1016416549682617\n",
      "val 2.151332139968872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((['Model 1', 2.1276261806488037, 2.1701834201812744],\n",
       "  ['Model 2:  b2*0; W2=W2*0.01', 2.069324493408203, 2.133209228515625]),\n",
       " ['Model 3:  W1=0.01; b2=0; W2=W2*0.01', None, None])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h =torch.tanh(embcat @ W1 + b1)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "tracking_stats = tracking_stats , ['Model 3:  W1=0.01; b2=0; W2=W2*0.01',split_loss('train'),split_loss('val')]\n",
    "tracking_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Problem gets worse, when you have more layers\n",
    "#### What is the value you should multiply W with to preserve a standard deviation of 1 for hpreact = embcat @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean x: -0.006351, Std x: 0.993653\n",
      "Mean w: -0.008941, Std w: 0.990991\n",
      "Mean y: 0.004615, Std y: 3.111047\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200)\n",
    "y = x @ w\n",
    "print(f'Mean x: {x.mean():.6f}, Std x: {x.std():.6f}')\n",
    "print(f'Mean w: {w.mean():.6f}, Std w: {w.std():.6f}')\n",
    "print(f'Mean y: {y.mean():.6f}, Std y: {y.std():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: * 1 / (10**0.5)\n",
    "#### We should also add a gain: multiply by (5/3) divide by fan_mode. no clue why\n",
    "#### So: * (5/3)/((n_embd * block_size)**0.5) #* 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean x: 0.002125, Std x: 1.001333\n",
      "Mean w: 0.019494, Std w: 0.313731\n",
      "Mean y: 0.000415, Std y: 0.994693\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = x @ w\n",
    "print(f'Mean x: {x.mean():.6f}, Std x: {x.std():.6f}')\n",
    "print(f'Mean w: {w.mean():.6f}, Std w: {w.std():.6f}')\n",
    "print(f'Mean y: {y.mean():.6f}, Std y: {y.std():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean x: -0.002520, Std x: 0.999163\n",
      "Mean w: -0.005806, Std w: 0.302183\n",
      "Mean y: 0.003868, Std y: 0.952729\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) * (5/3) / 30**0.5\n",
    "y = x @ w\n",
    "print(f'Mean x: {x.mean():.6f}, Std x: {x.std():.6f}')\n",
    "print(f'Mean w: {w.mean():.6f}, Std w: {w.std():.6f}')\n",
    "print(f'Mean y: {y.mean():.6f}, Std y: {y.std():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182441, 3]) torch.Size([182441])\n",
      "torch.Size([22902, 3]) torch.Size([22902])\n",
      "torch.Size([22803, 3]) torch.Size([22803])\n",
      "11897\n",
      "      0/ 200000: 3.2919\n",
      "  10000/ 200000: 2.3768\n",
      "  20000/ 200000: 2.2040\n",
      "  30000/ 200000: 2.1786\n",
      "  40000/ 200000: 2.0913\n",
      "  50000/ 200000: 2.0421\n",
      "  60000/ 200000: 1.9533\n",
      "  70000/ 200000: 2.2160\n",
      "  80000/ 200000: 1.9515\n",
      "  90000/ 200000: 2.0229\n",
      " 100000/ 200000: 1.8909\n",
      " 110000/ 200000: 1.8692\n",
      " 120000/ 200000: 2.0451\n",
      " 130000/ 200000: 2.0751\n",
      " 140000/ 200000: 1.7027\n",
      " 150000/ 200000: 2.0493\n",
      " 160000/ 200000: 1.6198\n",
      " 170000/ 200000: 1.7937\n",
      " 180000/ 200000: 2.1341\n",
      " 190000/ 200000: 1.7599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1112.,  188.,  152.,  118.,  115.,   94.,   81.,   65.,   63.,\n",
       "          51.,   45.,   50.,   65.,   54.,   69.,   59.,   64.,   62.,\n",
       "          81.,  100.,   83.,   78.,   57.,  123.,  111.,   67.,  112.,\n",
       "          82.,   64.,   74.,   86.,   71.,   64.,   54.,   49.,   48.,\n",
       "          49.,   55.,   79.,   45.,   68.,   80.,   80.,   97.,   83.,\n",
       "         106.,  131.,  170.,  255., 1191.]),\n",
       " array([-9.99999881e-01, -9.59999884e-01, -9.19999888e-01, -8.79999892e-01,\n",
       "        -8.39999895e-01, -7.99999899e-01, -7.59999902e-01, -7.19999906e-01,\n",
       "        -6.79999909e-01, -6.39999913e-01, -5.99999917e-01, -5.59999920e-01,\n",
       "        -5.19999924e-01, -4.79999927e-01, -4.39999931e-01, -3.99999934e-01,\n",
       "        -3.59999938e-01, -3.19999942e-01, -2.79999945e-01, -2.39999949e-01,\n",
       "        -1.99999952e-01, -1.59999956e-01, -1.19999959e-01, -7.99999630e-02,\n",
       "        -3.99999666e-02,  2.98023223e-08,  4.00000262e-02,  8.00000226e-02,\n",
       "         1.20000019e-01,  1.60000015e-01,  2.00000012e-01,  2.40000008e-01,\n",
       "         2.80000005e-01,  3.20000001e-01,  3.59999998e-01,  3.99999994e-01,\n",
       "         4.39999990e-01,  4.79999987e-01,  5.19999983e-01,  5.59999980e-01,\n",
       "         5.99999976e-01,  6.39999973e-01,  6.79999969e-01,  7.19999965e-01,\n",
       "         7.59999962e-01,  7.99999958e-01,  8.39999955e-01,  8.79999951e-01,\n",
       "         9.19999948e-01,  9.59999944e-01,  9.99999940e-01]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "# Activation: still looks bad though....\n",
    "plt.hist(h.view(-1).tolist(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.036036491394043\n",
      "val 2.109381675720215\n",
      "train 2.036036491394043\n",
      "val 2.109381675720215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((['Model 1', 2.1276261806488037, 2.1701834201812744],\n",
       "   ['Model 2:  b2*0; W2=W2*0.01', 2.069324493408203, 2.133209228515625]),\n",
       "  ['Model 3:  W1=0.01; b2=0; W2=W2*0.01', None, None]),\n",
       " ['Model 4:  W1=(5/3)/((n_embd * block_size)**0.5); b2=0; W2=W2*0.01',\n",
       "  None,\n",
       "  None])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h =torch.tanh(embcat @ W1 + b1)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "\n",
    "tracking_stats = tracking_stats , ['Model 4:  W1=(5/3)/((n_embd * block_size)**0.5); b2=0; W2=W2*0.01',split_loss('train'),split_loss('val')]\n",
    "tracking_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "#### We want the preactivation function not procuding values that are very small (tanh produces -1) neither way too large (tanh produces ones)\n",
    "#### We want them to be roughly Gaussian: subtract mean divide by standard deviation\n",
    "#### Addtionally to batch normalization: we want the network not to be forced to be Gaussian. So we want it to be able to shift and deviate from that distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182424, 3]) torch.Size([182424])\n",
      "torch.Size([22836, 3]) torch.Size([22836])\n",
      "torch.Size([22886, 3]) torch.Size([22886])\n",
      "12297\n",
      "      0/ 200000: 3.2926\n",
      "  10000/ 200000: 2.3681\n",
      "  20000/ 200000: 1.7444\n",
      "  30000/ 200000: 2.1972\n",
      "  40000/ 200000: 2.0636\n",
      "  50000/ 200000: 2.0787\n",
      "  60000/ 200000: 1.9219\n",
      "  70000/ 200000: 2.1091\n",
      "  80000/ 200000: 2.1248\n",
      "  90000/ 200000: 2.0234\n",
      " 100000/ 200000: 2.0432\n",
      " 110000/ 200000: 1.8459\n",
      " 120000/ 200000: 2.0609\n",
      " 130000/ 200000: 2.2648\n",
      " 140000/ 200000: 1.8508\n",
      " 150000/ 200000: 1.6568\n",
      " 160000/ 200000: 1.8573\n",
      " 170000/ 200000: 2.2071\n",
      " 180000/ 200000: 1.7515\n",
      " 190000/ 200000: 1.8627\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "# Addtionally to batch normalization: we want the network not to be forced to be Gaussian\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bngain]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 + b1\n",
    " \n",
    " \n",
    "  # Batch Normalization NEW\n",
    "  hpreact = bngain * (hpreact-hpreact.mean(0, keepdim= True) / hpreact.std(0, keepdim= True) ) - bnbias\n",
    " \n",
    " \n",
    " \n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0489697456359863\n",
      "val 2.1184422969818115\n",
      "train 2.0489697456359863\n",
      "val 2.1184422969818115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((((['Model 1', 2.1276261806488037, 2.1701834201812744],\n",
       "    ['Model 2:  b2*0; W2=W2*0.01', 2.069324493408203, 2.133209228515625]),\n",
       "   ['Model 3:  W1=0.01; b2=0; W2=W2*0.01', None, None]),\n",
       "  ['Model 4:  W1=(5/3)/((n_embd * block_size)**0.5); b2=0; W2=W2*0.01',\n",
       "   None,\n",
       "   None]),\n",
       " ['Model 5:  Batch Normalization', None, None])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact-hpreact.mean(0, keepdim= True) / hpreact.std(0, keepdim= True) ) - bnbias\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "tracking_stats = tracking_stats , ['Model 5:  Batch Normalization',split_loss('train'),split_loss('val')]\n",
    "tracking_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mondalynn.\n",
      "hegermed.\n",
      "rhal.\n",
      "remman.\n",
      "brerleg.\n",
      "adelynne.\n",
      "hillyn.\n",
      "jennedecklandraxleigh.\n",
      "mcky.\n",
      "adi.\n",
      "shdynr.\n",
      "shimier.\n",
      "kindrenevionnicocen.\n",
      "brence.\n",
      "rhylene.\n",
      "eli.\n",
      "kayshawn.\n",
      "lan.\n",
      "hal.\n",
      "sadyansh.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 3\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what we have achieved so far:\n",
    "\n",
    "Negative Loglikelihood\n",
    "| Step | Train | Test |\n",
    "| :- | -: | :-: |\n",
    "| Standard | 2.1276 | 2.1701 |\n",
    "| W2 * 0.01 and b2 * 0 | 2.2556 | 2.3397 |\n",
    "| + W1 * 0.01 and b1 * 0.01 | 2.1016 | 2.1513 |\n",
    "| equally distributed values and b1 * 0.01 | 2.1051 | 2.1384 |\n",
    "| Improving Gradient | 2.0377 | 2.1032 |\n",
    "| Batch Normalization | 2.0411 | 2.1137 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization has a regulaizing effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### achieving gaussian distribution is easy in a neural network with just one layer, but gets more complicated the more layers there are involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182512, 3]) torch.Size([182512])\n",
      "torch.Size([22860, 3]) torch.Size([22860])\n",
      "torch.Size([22774, 3]) torch.Size([22774])\n",
      "12097\n",
      "      0/ 200000: 3.2835\n",
      "  10000/ 200000: 2.4888\n",
      "  20000/ 200000: 2.5556\n",
      "  30000/ 200000: 2.3234\n",
      "  40000/ 200000: 2.1004\n",
      "  50000/ 200000: 2.4306\n",
      "  60000/ 200000: 2.1232\n",
      "  70000/ 200000: 1.7836\n",
      "  80000/ 200000: 2.1853\n",
      "  90000/ 200000: 2.1955\n",
      " 100000/ 200000: 1.7435\n",
      " 110000/ 200000: 1.9625\n",
      " 120000/ 200000: 2.2277\n",
      " 130000/ 200000: 2.0731\n",
      " 140000/ 200000: 2.0738\n",
      " 150000/ 200000: 1.9988\n",
      " 160000/ 200000: 2.1839\n",
      " 170000/ 200000: 2.4861\n",
      " 180000/ 200000: 2.0557\n",
      " 190000/ 200000: 2.0239\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "\n",
    "# Addtionally to batch normalization: we want the network not to be forced to be Gaussian\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running =  torch.ones((1, n_hidden))\n",
    "\n",
    "bnmean_running2 = torch.zeros((1, n_hidden))\n",
    "bnstd_running2 =  torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  hpreact = embcat @ W1 # + b1 # because b1 gets canceled out during batch normalization!\n",
    " \n",
    " \n",
    "  # Batch Normalization NEW\n",
    "  bnmeani = hpreact.mean(0, keepdim= True)\n",
    "  bnstdi = hpreact.std(0, keepdim= True) + 0.001  # add a small error so if the variance is zero, we are not running into troubles\n",
    "  \n",
    "  hpreact = bngain * (hpreact-bnmeani) / bnstdi - bnbias \n",
    " \n",
    "  with torch.no_grad():\n",
    "    bnmean_running2 = (bnmean_running*i + bnmeani) / (i+1)\n",
    "    bnstd_running2 = (bnstd_running*i + bnstdi) / (i+1)\n",
    "    bnmean_running = 0.999 * bnmean_running2 + 0.001 * bnmeani  # running mean: because we are trying to estimate the mean of the whole dataset. With a small batch size higher values might be to sensitive for outliers. starting at mean zero we are estimating what the next batch would change\n",
    "    bnstd_running = 0.999 * bnstd_running2 + 0.001 * bnstdi\n",
    "   \n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.3282535076141357\n",
      "val 2.377671718597412\n",
      "train 2.3282535076141357\n",
      "val 2.377671718597412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((((['Model 1', 2.1276261806488037, 2.1701834201812744],\n",
       "     ['Model 2:  b2*0; W2=W2*0.01', 2.069324493408203, 2.133209228515625]),\n",
       "    ['Model 3:  W1=0.01; b2=0; W2=W2*0.01', None, None]),\n",
       "   ['Model 4:  W1=(5/3)/((n_embd * block_size)**0.5); b2=0; W2=W2*0.01',\n",
       "    None,\n",
       "    None]),\n",
       "  ['Model 5:  Batch Normalization', None, None]),\n",
       " ['Model 5:  Batch Normalization', None, None])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact-hpreact.mean(0, keepdim= True) / hpreact.std(0, keepdim= True) ) - bnbias\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "tracking_stats = tracking_stats , ['Model 5:  Batch Normalization',split_loss('train'),split_loss('val')]\n",
    "tracking_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration for prediction\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim= True)\n",
    "    bnstd = hpreact.std(0, keepdim= True)\n",
    "\n",
    "plt.scatter(bnstd_running, bnstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bnmean_running2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact-hpreact.mean(0, keepdim= True) / hpreact.std(0, keepdim= True) ) - bnbias\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "block_size = 3\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01  # scale the weight close to zero (not exactly zero because: )\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  # for initialization the bias shoud be zero (or equal) -> assumption: uniform distribution of all the characters\n",
    "print(b2)\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_genai",
   "language": "python",
   "name": "venv_genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
